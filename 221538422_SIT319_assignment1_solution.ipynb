{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyPstzaeessdXzuYUU4xjowH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvanhieu/SIT315_DEAKIN/blob/main/221538422_SIT319_assignment1_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Student Information:**\n",
        "\n",
        "Student Name: Van Hieu Nguyen\n",
        "\n",
        "Student ID: 221538422\n",
        "\n",
        "Unit Name: S319 - Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "# Assignment 1: Deep Learning (Week 1 to Week 3)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "EOWEJbEk4cCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "9cfMAO0q5rks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project explores the step-by-step implementation, optimization, and evaluation of a neural network model for FashionMNIST classification. The project progresses through four key stages, each building upon the previous one to improve performance and accuracy score."
      ],
      "metadata": {
        "id": "a2Ruy6Yy51Ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "zD3bLnvi51mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set 1: Build a Simple Neural Network"
      ],
      "metadata": {
        "id": "OlzCOq8a5KKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 1. problem Definition"
      ],
      "metadata": {
        "id": "NXe3auoOIJG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to construct and train a basic neural network for the classification of photos from the Fashion-MNIST dataset, which serves as a contemporary substitute for the original MNIST dataset. The Fashion-MNIST dataset comprises 60,000 training pictures and 10,000 test images, each measuring 28x28 pixels, depicting grayscale representations of several fashion products. The dataset has 10 categories representing various types of apparel, including T-shirts, pants, dresses, and footwear.\n",
        "\n",
        "**Objective:**\n",
        "*   To construct a neural network capable of accurately categorising these photos into one of the ten specified classifications.\n",
        "<table border=\"1\">\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Description</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "*   Assess the model's efficacy and enhance its precision with a basic feedforward neural network.\n",
        "\n",
        "**Dataset Limitations:**\n",
        "*   Class Imbalance: The dataset exhibits little class imbalance, rendering it more appropriate for training purposes.\n",
        "*   Ethical Consideration: Fashion-MNIST contains images of clothing items, so there is no inherent ethical issue with the dataset.\n",
        "\n",
        "**Plan to Implementation:**\n",
        "*   Set 1: Loading dataset, Train a neural network to classify FashionMNIST images\n",
        "*   Set 2: Optimize model performance using better architectures, activation functions, and optimizers. It relates to TensorBoard, Training Parameter, and Model Performance.\n",
        "*   Set 3: Analyze dataset biases and apply mitigation techniques and hyperparemeter.\n",
        "*   Set 4: deeper exploration of hyperparameters, and Grokking paper using a basic algorithmic dataset and training dynamics."
      ],
      "metadata": {
        "id": "tf9Ri00AJoDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 2. Dataset Selection and Preprocessing"
      ],
      "metadata": {
        "id": "SLwosg1zId-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset:** The Fashion-MNIST dataset was chosen due to its accessibility and use as a standard benchmark in image classification tasks.\n",
        "**Preprocessing:**\n",
        "*   Normalization: Pixel values in the images range from 0 to 255, so they are normalized to a range of [0, 1] by dividing each pixel value by 255.\n",
        "*   Train-Test Split: The dataset is already split into training and test sets, with 60,000 images for training and 10,000 for testing. No additional splitting is necessary.\n",
        "*   One-hot Encoding: the labels are converted into one-hot encoding, where each class label is represented as a vector of 0s with a 1 at the index corresponding to the class."
      ],
      "metadata": {
        "id": "TNVKmIy2YYIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard\n",
        "import time\n",
        "import random\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "yi9JSgFrBB31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n",
        "    torch.backends.cudnn.benchmark = False     # Ensures consistent performance\n",
        "\n",
        "set_seed(42)  # Set the seed before training\n",
        "\n",
        "# Configure device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "sYMRXH8eCc5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "id": "PBE3qWS_mLBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to have values between -1 and 1\n",
        "])"
      ],
      "metadata": {
        "id": "wgiwhbjEBB56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load the training dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test dataset\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "chlfb_L9BJub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to show images\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images))\n"
      ],
      "metadata": {
        "id": "wsfESjXRBJw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Labels: {labels}')"
      ],
      "metadata": {
        "id": "R_JIBnHqBJzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 3. Neural Network Implementation"
      ],
      "metadata": {
        "id": "GCKX3nv0IyUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The network consists of:\n",
        "\n",
        "*   Input layer: The input is a flattened 28x28 image, resulting in a vector of 784 features.\n",
        "*   Hidden layers: Two fully connected hidden layers with ReLU activation to introduce non-linearity and enable the model to learn complex patterns.\n",
        "*   Output layer: A softmax output layer with 10 units, one for each class in the dataset, to produce the final class probabilities.\n",
        "\n",
        "Network Architecture:\n",
        "*   Input Layer: 784 neurons (28x28 image flattened)\n",
        "*   Hidden Layer 1: 128 neurons with ReLU activation\n",
        "*   Hidden Layer 2: 64 neurons with ReLU activation\n",
        "*   Output Layer: 10 neurons with softmax activation"
      ],
      "metadata": {
        "id": "x5ozN2QXY1OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network class\n",
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer to first hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)  # First hidden layer to second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)  # Second hidden layer to output layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the image into a vector\n",
        "        x = self.relu(self.fc1(x))  # First hidden layer with ReLU\n",
        "        x = self.relu(self.fc2(x))  # Second hidden layer with ReLU\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return self.softmax(x)  # Softmax activation for multi-class classification\n",
        "\n",
        "# Initialize the model\n",
        "model = FashionMNISTModel().to(device)"
      ],
      "metadata": {
        "id": "pGGZPnPoI-RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 4. Training Pipeline"
      ],
      "metadata": {
        "id": "1glVQA30JDxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the training Pipeline involves:\n",
        "*   Defining a loss function (Mean Squared Error Lossn).\n",
        "*   Using an optimizer (Stochastic Gradient Descent) to update model weights.\n",
        "*   Iterating over the training dataset multiple times (epochs) to minimize the loss.\n",
        "*   Evaluating the model on the test set after each epoch to track progress."
      ],
      "metadata": {
        "id": "4TDx3SSUJRQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter(log_dir='runs/fashion_mnist_experiment')  # Logs for TensorBoard\n",
        "\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD optimizer\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Training function with results printed every 10 epochs\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
        "    best_accuracy = 0  # Track best accuracy\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Convert labels to one-hot encoding\n",
        "            labels_one_hot = torch.eye(10, device=labels.device)[labels]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels_one_hot)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        test_acc = evaluate_model(model, test_loader)\n",
        "\n",
        "        # Log loss and accuracy to TensorBoard\n",
        "        writer.add_scalar('Loss/train', avg_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Training completed in {(end_time - start_time):.2f} seconds!\")\n",
        "    return test_acc\n",
        "    writer.close()  # Close TensorBoard writer\n",
        "# Run training with flexible num_epochs\n",
        "num_epochs = 10\n",
        "train(model, train_loader, test_loader, criterion, optimizer, num_epochs=num_epochs)\n",
        "# Print accuracy result\n",
        "print(f\"Model Accuracy on Test Dataset: {evaluate_model(model, test_loader)}%\")"
      ],
      "metadata": {
        "id": "REaV2NvMJDEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is evaluated on the test set. The evaluation metrics include:\n",
        "*   Accuracy: The percentage of correctly predicted labels on the test set.\n",
        "*   Loss: The value of the cross-entropy loss, indicating the model's performance."
      ],
      "metadata": {
        "id": "3aEOXtPfZgVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set 2: Improve Model Performance"
      ],
      "metadata": {
        "id": "vl2bVKwK5UOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 1. Logging and Visualization"
      ],
      "metadata": {
        "id": "UYjp9XB9CbG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard"
      ],
      "metadata": {
        "id": "FtGhPp-pvqIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "R3XwPiZivqK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "xHxKDBVuvqNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Model and Training Adjustments"
      ],
      "metadata": {
        "id": "T3UpRfzO8wyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Added a third hidden layer\n",
        "*   Replaced ReLU with Leaky ReLU\n",
        "*   Used Adam optimizer\n",
        "*   Lowered Learning Rate (0.001 instead of 0.01)"
      ],
      "metadata": {
        "id": "LlgyEF-BfIG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New network architecture with an additional hidden layer\n",
        "class ImprovedFashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedFashionMNISTModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer to first hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)  # First hidden layer to second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 32)  # New additional hidden layer\n",
        "        self.fc4 = nn.Linear(32, 10)  # Second hidden layer to output layer\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)  # Leaky ReLU activation\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the image into a vector\n",
        "        x = self.leaky_relu(self.fc1(x))  # First hidden layer with ReLU\n",
        "        x = self.leaky_relu(self.fc2(x))  # Second hidden layer with ReLU\n",
        "        x = self.leaky_relu(self.fc3(x))  # Third hidden layer with ReLU\n",
        "        x = self.fc4(x)  # Output layer\n",
        "        return self.softmax(x)  # Softmax activation for multi-class classification\n",
        "\n",
        "# Run the updated model with new configurations\n",
        "model = ImprovedFashionMNISTModel().to(device)"
      ],
      "metadata": {
        "id": "Rk__sHd8_VHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify training configurations: Learning Rate and Batch Size:\n",
        "\n",
        "# New optimizer configuration with modified learning rate and batch size\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with lower learning rate\n",
        "\n",
        "# DataLoader with batch size set to 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "twq5jLNd_VJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the update model:\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter(log_dir='runs/fashion_mnist_experiment2')  # Logs for TensorBoard\n",
        "\n",
        "train(model, train_loader, test_loader, criterion, optimizer, num_epochs=10)\n"
      ],
      "metadata": {
        "id": "kQsEy5Cp_VMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print accuracy result\n",
        "print(f\"Model Accuracy on Test Dataset: {evaluate_model(model, test_loader)}%\")"
      ],
      "metadata": {
        "id": "PLGXtj5hsKsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Comparative Analysis:"
      ],
      "metadata": {
        "id": "tqp2a5df82Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "k3Qk7WN_MzCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss between two models:**\n",
        "\n",
        "1.   Accuracy Score Improved Significantly:\n",
        "*   The improved model (Set 2) reached 90.58% training accuracy after 10 epochs, compared to 85.70% in Set 1.\n",
        "*   he test accuracy increased from 84.05% → 88.10%, indicating better generalization.\n",
        "2.   Faster decreased loss: In Set 1, the loss started high (0.0740) and decreased gradually. While, the loss started lower (0.0281) and decreased faster in set 2, showing a more efficient learning process.\n",
        "3.   Leaky ReLU Helped Reduce Vanishing Gradient:\n",
        "*   Set 1 (using ReLU): May suffer from dead neurons (where neurons stop updating).\n",
        "*   Set 2 (using Leaky ReLU): Allowed small gradients in inactive neurons, leading to more stable training.\n",
        "4.   Adam Optimizer make training pipeline faster and smoother convergence.\n",
        "\n",
        "**The improved model helps performed better:**\n",
        "*   Added a additional Hidden Layer allows the model to learn better features\n",
        "*   Using Leaky ReLU: prevented dead neurons, leading to more stable training\n",
        "*   Using Adam for optimizing: it helps faster convergence, dynamically adjusted learning rates.\n",
        "*   Applied lower Learning rate: Allowed for more precise weight updates, reducing oscillations\n",
        "\n",
        "Therefore, The improved model (Set 2) trained faster, learned more efficiently, and generalized better, leading to higher test accuracy (88.10%) compared to the original model (84.05%).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lmZbGdP-MsQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set 3: Ethical Analysis and Model Evaluation"
      ],
      "metadata": {
        "id": "UhA2Ig_95aLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset Bias and Limitations"
      ],
      "metadata": {
        "id": "1tTdhI7Thkmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences of each class\n",
        "#class_counts = Counter(train_dataset.targets.numpy())\n",
        "class_counts = np.bincount(train_dataset.targets.numpy())\n",
        "class_labels = train_dataset.classes\n",
        "\n",
        "# Plot class distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(class_labels, class_counts, color='skyblue', edgecolor='black')\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Class Distribution in Training Set\")\n",
        "plt.show()\n",
        "\n",
        "# Print class counts\n",
        "for label, count in zip(class_labels, class_counts):\n",
        "    print(f\"{label}: {count} samples\")\n"
      ],
      "metadata": {
        "id": "-udGWmaXWw_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This plot shows that all classes have nearly identical counts, indicating a perfect balanced in Fashion MNIST dataset.\n",
        "- The dataset might not fully represent real-world clothing variations, such as different angles, lighting conditions, or worn-out clothes.\n",
        "- There is no class imbalance issues:\n",
        "Since all classes have an equal number of samples, the model is unlikely to be biased toward any particular class.\n",
        "- Impact on Model Performance:\n",
        "\n",
        "  *   Since the dataset is balanced, accuracy will not be misleading\n",
        "  *   If the dataset lacks diversity in variations, the model might fail when encountering new styles, patterns, or real-world conditions.\n",
        "\n"
      ],
      "metadata": {
        "id": "7xJFXZbKpMVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Mitigation Techniques"
      ],
      "metadata": {
        "id": "sLuNhoNihrcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Oversampling:  involves increasing the number of training examples in underrepresented classes by duplicating them."
      ],
      "metadata": {
        "id": "9nxBykaBsNXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import torch\n",
        "\n",
        "# Convert dataset to numpy for oversampling\n",
        "X_train = train_dataset.data.numpy().reshape(-1, 28 * 28)  # Flatten images\n",
        "y_train = train_dataset.targets.numpy()\n",
        "\n",
        "# Apply oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X_resampled = torch.tensor(X_resampled, dtype=torch.float32).view(-1, 1, 28, 28)\n",
        "y_resampled = torch.tensor(y_resampled, dtype=torch.long)\n",
        "\n",
        "# Create a new balanced dataset\n",
        "balanced_train_dataset = torch.utils.data.TensorDataset(X_resampled, y_resampled)\n",
        "balanced_train_loader = DataLoader(balanced_train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "print(\"Oversampling Complete! the dataset size:\", len(balanced_train_dataset))\n",
        "\n",
        "# Original class distribution\n",
        "original_counts = Counter(train_dataset.targets.numpy())\n",
        "print(\"Original class distribution:\", original_counts)"
      ],
      "metadata": {
        "id": "MLP62nzGg9JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a batch from the new balanced dataset\n",
        "data_iter = iter(balanced_train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Display images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 6, figsize=(10, 3))\n",
        "for i in range(6):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
        "    ax.set_title(f\"Label: {labels[i].item()}\")\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lWNvY15ZjjsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights (inverse frequency)\n",
        "class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)\n",
        "class_weights = class_weights / class_weights.sum()  # Normalize\n",
        "\n",
        "# Move weights to device\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# Define weighted loss function\n",
        "criterion_weighted = nn.CrossEntropyLoss(weight=class_weights)\n"
      ],
      "metadata": {
        "id": "gjDUhmUhg9Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Train and Compare the performance and Evaluate Model"
      ],
      "metadata": {
        "id": "jmguyrv3s-ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model and optimizer\n",
        "model = FashionMNISTModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train on original imbalanced dataset\n",
        "test_acc_before = train(model, train_loader, test_loader, criterion, optimizer, 5)\n",
        "\n",
        "print(f\"\\nTest Accuracy Before Bias Mitigation: {test_acc_before}%\")\n"
      ],
      "metadata": {
        "id": "xofL0fzig9UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define new model\n",
        "model_balanced = FashionMNISTModel().to(device)\n",
        "optimizer_balanced = optim.Adam(model_balanced.parameters(), lr=0.0005)\n",
        "\n",
        "# Train on balanced dataset\n",
        "test_acc_oversampling = train(model_balanced, balanced_train_loader, test_loader, criterion, optimizer_balanced, 5)\n",
        "\n",
        "print(f\"\\nTest Accuracy After Oversampling: {test_acc_oversampling}%\")\n"
      ],
      "metadata": {
        "id": "GnPBUFbxg9Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Compare the performance and Discussion the output:"
      ],
      "metadata": {
        "id": "rNx1UNwfzI7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This part applied oversampling to FashionMNIST dataset, however, FashionMNIST is already balanced, and each class has 6000 images. The \"oversampling\" method may have created duplicate data, which caused the model to overfit. This lead to Test Accuracy is dropped significantly (from 87.36% to 22.25%)\n",
        "\n",
        "- The original model was trained on diverse samples. But, The oversampled model was trained on the same images multiple times, making it memorize training data instead of generalizing.\n",
        "\n",
        "- Therefore, the solution for this FashionMNIST dataset is remove oversampling and train on the original dataset with neural network\n"
      ],
      "metadata": {
        "id": "OmmnDDX-zVb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Critical Reflection\n",
        "\n",
        "  a. Strengths:\n",
        "  *   The model achived 87.36 percent test accuracy, which is a strong result for FashionMINIST, this also shows that the model is not overfitting or underfitting.\n",
        "  *   FashionMINIST dataset is already balanced dataset with 6000 images, and 10 classifications, each type has 6000 images. Therefore, this model dont need to applied oversampling technique.\n",
        "\n",
        "  b. Limitations:\n",
        "  *   Unnecessary Oversampling: it led to overfitting and poor test accuracy (22.25%) because the model saw duplicate data.\n",
        "  *   Data Integrity issues: The significant drop in test accuracy suggests possible corrupted data or duplication data after oversampling.\n",
        "\n",
        "  c. Improve:\n",
        "  *   Avoid Unnecessary Oversampling:  oversampling was not needed for balanced dataset, it just make our model incorrect. Therefore, the model can improve by focusing on data augmentation to introduce variety without duplication.\n",
        "  *   Test Multiple Bias Mitigation Techniques: such as Data augmentation (random rotations, flips, noise), Class-weighted loss functions (to handle minor class imbalances), Ensemble learning (to improve robustness)\n",
        "  * Debugging: check data integrity such as labels, samples, or data distribution."
      ],
      "metadata": {
        "id": "ZdzXANMNhvui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improve the model using Class-weighted loss functions: the dataset have slightly class imbalances, so try to enhance the model by applying Class-weighted loss functions"
      ],
      "metadata": {
        "id": "uxuPtQHYcfZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# Compute class counts\n",
        "class_counts = Counter(y_train)\n",
        "total_samples = sum(class_counts.values())\n",
        "\n",
        "# Compute weights: Inverse frequency (higher weight for underrepresented classes)\n",
        "class_weights = {label: total_samples / count for label, count in class_counts.items()}\n",
        "weights = torch.tensor([class_weights[i] for i in range(10)], dtype=torch.float32)\n",
        "\n",
        "# Define weighted loss function\n",
        "criterion_weighted = torch.nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "def train_with_weighted_loss(model, train_loader, test_loader, optimizer, num_epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Compute weighted loss\n",
        "            loss = criterion_weighted(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"Training completed!\")\n"
      ],
      "metadata": {
        "id": "JNLNrgvrsQUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = FashionMNISTModel().to(device)\n",
        "optimizers = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "train_with_weighted_loss(model, train_loader, test_loader, optimizers, num_epochs=5)\n",
        "print(f\"Test Accuracy: {evaluate_model(model, test_loader)}%\")"
      ],
      "metadata": {
        "id": "p1M02P9LETJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows even there is a higher loss value, but the accuracy score is reached 90.83 percent and the test accuracy 88.15. This is better score compared with the original model and bias mitigation using oversampling technique."
      ],
      "metadata": {
        "id": "lrYatPU2kRGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set 4: Reproducing and Analyzing \"Grokking\" in Neural Network\n",
        "\n",
        "Implemnet a simple algorithm task from Grokking paper using modular addition"
      ],
      "metadata": {
        "id": "pCU5e1ds5fbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define dataset:"
      ],
      "metadata": {
        "id": "3yzhoahtMeqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Define prime number for modular arithmetic\n",
        "MODULO = 97\n",
        "\n",
        "# Generate modular addition dataset\n",
        "def generate_data(size, modulo=MODULO):\n",
        "    data = []\n",
        "    for _ in range(size):\n",
        "        a = random.randint(0, modulo - 1)\n",
        "        b = random.randint(0, modulo - 1)\n",
        "        c = (a + b) % modulo  # Modular addition\n",
        "        data.append((a, b, c))\n",
        "    return data\n",
        "\n",
        "# Split dataset\n",
        "total_data = generate_data(20000)\n",
        "train_size = int(0.8 * len(total_data))  # Use 80% of the data for training\n",
        "train_data = total_data[:train_size]\n",
        "val_data = total_data[train_size:]\n",
        "\n",
        "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}\")"
      ],
      "metadata": {
        "id": "W10uptdq5l2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create PyTorch function and dataset loader"
      ],
      "metadata": {
        "id": "ScneggoBMxou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModularAdditionDataset(Dataset):\n",
        "    def __init__(self, data, modulo):\n",
        "        self.data = data\n",
        "        self.modulo = modulo\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        a, b, c = self.data[idx]\n",
        "        a_onehot = torch.eye(self.modulo)[a]\n",
        "        b_onehot = torch.eye(self.modulo)[b]\n",
        "        c_onehot = torch.eye(self.modulo)[c]  # One-hot encoding of result\n",
        "        return torch.cat((a_onehot, b_onehot)), c_onehot\n",
        "\n",
        "# Create dataset loaders\n",
        "batch_size = 128\n",
        "train_dataset = ModularAdditionDataset(train_data, MODULO)\n",
        "val_dataset = ModularAdditionDataset(val_data, MODULO)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "VyROacAs5qIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define the Neural Network Model"
      ],
      "metadata": {
        "id": "8LSkXrytNJ-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModularAdditionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(ModularAdditionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return self.fc2(x)  # No softmax since we'll use CrossEntropyLoss\n",
        "\n",
        "# Initialize model\n",
        "hidden_size = 128\n",
        "model = ModularAdditionModel(input_size=MODULO * 2, hidden_size=hidden_size, output_size=MODULO)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "4jgs5GYlNGWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Train and Evaluate The Model:"
      ],
      "metadata": {
        "id": "jAUIKTG9NSj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, optimizer, num_epochs=100):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets.argmax(dim=1))  # CrossEntropy requires integer labels\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Compute validation loss\n",
        "        val_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets.argmax(dim=1))\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        model.train()\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "        if epoch % 10 == 0:  # Print progress every 100 epochs\n",
        "            print(f\"Epoch {epoch}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train model and observe generalization\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "train_losses, val_losses = train(model, train_loader, val_loader, optimizer)\n"
      ],
      "metadata": {
        "id": "qqbOpAJ5U22h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: Training loss dropped to 0.0000 quickly → Suggests too much memorization (overfitting). However, the validation value is slowly dropped."
      ],
      "metadata": {
        "id": "oyNlGilhjr-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss:\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_losses, label='Train Loss', color='blue')\n",
        "plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss (Grokking)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P7HuQvmKPKu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Hyperparameter Exploration:"
      ],
      "metadata": {
        "id": "qLnqh66pPUZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with lower learning rate\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=1e-5)  # Lower learning rate\n",
        "train_losses_lr, val_losses_lr = train(model, train_loader, val_loader, optimizer)\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(val_losses, label='Val Loss (LR=0.01)', color='red')\n",
        "plt.plot(val_losses_lr, label='Val Loss (LR=0.001)', color='green')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Impact of Learning Rate on Grokking')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y1sP1mWkPaxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: this pipeline training shows that the model is better learning, compared beforing apply hyperparameter."
      ],
      "metadata": {
        "id": "BGlxZsAVlAx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss**:\n",
        "\n",
        "*   The tuned model generalized much faster, meaning the onset of grokking happened earlier.\n",
        "*   Validation Loss is much Lower with RMSprop hyperparameter. So, The model not only generalizes faster but also reaches a much lower error.\n",
        "*   Training Loss Stays Higher with RMSprop, this means the model focused on generalization instead of memorization. Therefore, hyperparameter helps avoid extreme overfitting, making the model more robust.\n"
      ],
      "metadata": {
        "id": "NRV0OHPjixQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Report and Reflection:\n",
        "\n",
        "\n",
        "Initially, the model memorized the training data quickly, as shown by the rapid drop in training loss. However, the validation loss remained high for many epochs before eventually decreasing, indicating a late onset of generalization (grokking). After hyperparameter tuning, the model generalized much earlier, with a lower validation loss overall and a more stable learning curve.\n",
        "\n",
        "Impact of Hyperparameter Changes on Training Dynamics\n",
        "\n",
        "\n",
        "*   Switching to RMSprop: it help the model update weights more effectively, preventing extreme memorization. This Led to a slower, smoother descent in training loss, allowing generalization.\n",
        "\n",
        "*   Reducing Learning Rate Dynamically: it helps avoid overfitting. this makes the validation loss dropped earlier, and  better generalization.\n",
        "\n",
        "*   Increasing Batch Size: it's reduced fluctuations in validation loss, making training more stable.\n",
        "\n",
        "Implications for the Grokking Effect:\n",
        "\n",
        "*   Before tuning, the model took a long time to generalize after overfitting. After tuning, grokking happened earlier, proving that careful hyperparameter selection influences when a model transitions from memorization to generalization.\n",
        "*   The results suggest that learning rate schedules and optimizers play a key role in grokking."
      ],
      "metadata": {
        "id": "-D0U1To1kFYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:"
      ],
      "metadata": {
        "id": "znaMTSEQ54RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This report demonstrated how deep learning models evolve with architectural changes, hyperparameter tuning, and training optimizations. By incorporating ethical analysis and bias mitigation, we ensured that the model not only performs well but also generalizes fairly. Additionally, the grokking experiment provided valuable insights into how models transition from memorization to generalization over time.\n",
        "The future improvements:\n",
        "*   Using CNNs instead of fully connected layers for better feature extraction.\n",
        "*   Applying batch normalization and dropout to improve stability and prevent overfitting.\n",
        "*   Experimenting with different bias mitigation techniques, such as data augmentation for underrepresented classes."
      ],
      "metadata": {
        "id": "33_kGgGVlbQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References:"
      ],
      "metadata": {
        "id": "B7zhVp9saVxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset: Fashion-MNIST. https://github.com/zalandoresearch/fashion-mnist?tab=readme-ov-file#get-the-data\n",
        "2. Code cademy: \"Activation Functions in PyTorch\". https://www.codecademy.com/resources/docs/pytorch/nn/activation-functions\n",
        "3. Yadav A (05 November 2024), \"ReLU vs LeakyReLU vs PReLU in PyTorch: A Deep Dive with Code Examples\", Medium. https://medium.com/%40amit25173/relu-vs-leakyrelu-vs-prelu-in-pytorch-a-deep-dive-with-code-examples-960172123834\n",
        "4. Geeks forgeeks, \"Handling Class Imbalance in PyTorch\". https://www.geeksforgeeks.org/handling-class-imbalance-in-pytorch/#3-weighted-random-sampler"
      ],
      "metadata": {
        "id": "Jo4bFxLQ07hw"
      }
    }
  ]
}